```python
# /core/config.py

import enum
from typing import List

from pydantic_settings import BaseSettings


class AppEnvironment(str, enum.Enum):
    PRODUCTION = "production"
    DEVELOPMENT = "development"
    TESTING = "testing"


class Settings(BaseSettings):
    """
    Application configuration settings.
    Reads settings from environment variables.
    """
    # Core settings
    APP_NAME: str = "EMR-System-Backend"
    APP_VERSION: str = "0.1.0"
    ENVIRONMENT: AppEnvironment = AppEnvironment.DEVELOPMENT
    API_V1_STR: str = "/api/v1"
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60

    # CORS settings
    BACKEND_CORS_ORIGINS: List[str] = ["https://localhost:3000"]

    # Database settings
    DATABASE_URL: str  # e.g., "postgresql+asyncpg://user:password@host:port/dbname"

    # S3 Storage settings
    S3_BUCKET_NAME: str
    S3_REGION: str
    AWS_ACCESS_KEY_ID: str
    AWS_SECRET_ACCESS_KEY: str

    class Config:
        env_file = ".env"
        case_sensitive = True


settings = Settings()

# /core/security.py

from datetime import datetime, timedelta
from typing import List, Optional

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from pydantic import BaseModel, ValidationError

from core.config import settings

ALGORITHM = "HS256"
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{settings.API_V1_STR}/auth/token")


class TokenData(BaseModel):
    """Schema for data contained within a JWT."""
    user_id: str
    roles: List[str] = []
    exp: Optional[int] = None


class AuthenticatedUser(BaseModel):
    """Schema representing the authenticated user, derived from the token."""
    id: str
    roles: List[str]


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    """Creates a new JWT access token."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt


async def get_current_user(token: str = Depends(oauth2_scheme)) -> AuthenticatedUser:
    """
    Decodes and validates JWT, returns the authenticated user model.
    Raises HTTPException for invalid tokens or credentials.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[ALGORITHM])
        token_data = TokenData(**payload)
        if token_data.exp is None or datetime.fromtimestamp(token_data.exp) < datetime.utcnow():
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired",
                headers={"WWW-Authenticate": "Bearer"},
            )
    except (JWTError, ValidationError):
        raise credentials_exception

    return AuthenticatedUser(id=token_data.user_id, roles=token_data.roles)


class RoleChecker:
    """
    Dependency to check if the authenticated user has the required roles.
    """
    def __init__(self, allowed_roles: List[str]):
        self.allowed_roles = allowed_roles

    def __call__(self, user: AuthenticatedUser = Depends(get_current_user)):
        if not any(role in self.allowed_roles for role in user.roles):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="The user does not have adequate privileges."
            )
        return user


# /db/session.py

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base

from core.config import settings

engine = create_async_engine(settings.DATABASE_URL, pool_pre_ping=True, echo=False)
AsyncSessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine,
    class_=AsyncSession,
    expire_on_commit=False,
)

Base = declarative_base()


async def get_db() -> AsyncSession:
    """Dependency to get an async database session."""
    async with AsyncSessionLocal() as session:
        yield session


# /db/models/clinical_document.py

import uuid
from sqlalchemy import Column, String, DateTime, ForeignKey, Enum
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.sql import func
import enum

from db.session import Base


class DocumentStatus(str, enum.Enum):
    UPLOADED = "uploaded"
    VERIFIED = "verified"
    ARCHIVED = "archived"


class ClinicalDocument(Base):
    """SQLAlchemy model for clinical document metadata."""
    __tablename__ = "clinical_documents"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    patient_id = Column(String, index=True, nullable=False)
    document_name = Column(String, nullable=False)
    document_type = Column(String, nullable=False)
    s3_key = Column(String, unique=True, nullable=False)
    s3_bucket = Column(String, nullable=False)
    file_size_bytes = Column(String, nullable=False)
    mime_type = Column(String, nullable=False)
    status = Column(Enum(DocumentStatus), default=DocumentStatus.UPLOADED, nullable=False)
    uploaded_by_user_id = Column(String, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False)


# /services/audit_logger.py

import logging
import json
from datetime import datetime, timezone
from typing import Dict, Any, Optional

# In a real system, this would write to a dedicated, immutable audit log service or database table.
# For this example, we use a structured logger.
logging.basicConfig(level=logging.INFO, format='%(message)s')
audit_logger = logging.getLogger("HIPAA_AUDIT")


class AuditEvent(str, enum.Enum):
    PHI_ACCESS = "PHI_ACCESS"
    PHI_CREATE = "PHI_CREATE"
    PHI_UPDATE = "PHI_UPDATE"
    PHI_DELETE = "PHI_DELETE"
    LOGIN_SUCCESS = "LOGIN_SUCCESS"
    LOGIN_FAILURE = "LOGIN_FAILURE"
    AUTHORIZATION_FAILURE = "AUTHORIZATION_FAILURE"


def log_audit_event(
    event: AuditEvent,
    user_id: str,
    details: Dict[str, Any],
    client_ip: Optional[str] = None,
    trace_id: Optional[str] = None,
):
    """
    Logs a security-relevant audit event in a structured format.
    """
    log_entry = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "event": event.value,
        "actor": {"userId": user_id},
        "source": {"ipAddress": client_ip},
        "traceId": trace_id,
        "details": details,
    }
    audit_logger.info(json.dumps(log_entry))


# /services/s3_service.py

import boto3
from botocore.exceptions import BotoCoreError, ClientError
from fastapi import UploadFile, HTTPException, status
from typing import Dict

from core.config import settings

s3_client = boto3.client(
    "s3",
    region_name=settings.S3_REGION,
    aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
    aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY
)


class S3Service:
    """Service for handling file uploads to an encrypted S3 bucket."""
    @staticmethod
    async def upload_file(
        *,
        file: UploadFile,
        patient_id: str,
        document_id: str
    ) -> Dict[str, str]:
        """
        Uploads a file to S3 with server-side encryption.
        Returns a dictionary containing the S3 key and bucket name.
        """
        s3_key = f"patients/{patient_id}/documents/{document_id}/{file.filename}"
        try:
            s3_client.upload_fileobj(
                file.file,
                settings.S3_BUCKET_NAME,
                s3_key,
                ExtraArgs={
                    "ServerSideEncryption": "AES256",
                    "ContentType": file.content_type
                }
            )
            return {"s3_key": s3_key, "s3_bucket": settings.S3_BUCKET_NAME}
        except (ClientError, BotoCoreError) as e:
            # In a real system, add more robust error handling and logging
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Could not upload file to storage: {str(e)}"
            )


# /api/v1/schemas/clinical_document.py

from pydantic import BaseModel, Field
from datetime import datetime
import uuid

from db.models.clinical_document import DocumentStatus


class DocumentCreate(BaseModel):
    """Schema for metadata provided during document upload."""
    document_type: str = Field(..., description="Type of the clinical document, e.g., 'Lab Report'.")


class DocumentResponse(BaseModel):
    """Schema for the response after a document is successfully created."""
    id: uuid.UUID
    patient_id: str
    document_name: str
    document_type: str
    file_size_bytes: int
    mime_type: str
    status: DocumentStatus
    uploaded_by_user_id: str
    created_at: datetime

    class Config:
        orm_mode = True


# /api/v1/endpoints/clinical_documents.py

import uuid
from fastapi import APIRouter, Depends, UploadFile, File, Form, HTTPException, status, Request
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import SQLAlchemyError

from core.security import AuthenticatedUser, RoleChecker
from db.session import get_db
from db.models.clinical_document import ClinicalDocument
from services.s3_service import S3Service
from services.audit_logger import log_audit_event, AuditEvent
from api.v1.schemas.clinical_document import DocumentResponse, DocumentCreate

router = APIRouter()

# Define roles with permission to upload documents.
# In a real system, this would be more granular (e.g., per-patient access).
authorized_uploader = RoleChecker(allowed_roles=["clinician", "nurse", "admin"])
MAX_FILE_SIZE_MB = 25
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_MB * 1024 * 1024
ALLOWED_MIME_TYPES = ["application/pdf", "image/jpeg", "image/png", "text/xml"]

@router.post(
    "/patients/{patient_id}/documents",
    response_model=DocumentResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Upload a Clinical Document",
    description="Uploads a clinical document for a specific patient. Requires appropriate clinical roles."
)
async def upload_clinical_document(
    request: Request,
    patient_id: str,
    file: UploadFile = File(...),
    metadata: DocumentCreate = Depends(),
    db: AsyncSession = Depends(get_db),
    current_user: AuthenticatedUser = Depends(authorized_uploader)
):
    """
    Handles the multipart/form-data upload of a clinical document.
    1. Validates file size and type.
    2. Generates a unique ID for the document.
    3. Uploads the file to a secure, encrypted S3 bucket.
    4. Saves document metadata to the PostgreSQL database.
    5. Creates a detailed audit log entry.
    """
    trace_id = request.headers.get("X-Request-ID")

    # 1. Validate file properties
    if not file.content_type in ALLOWED_MIME_TYPES:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, "File type not permitted.")

    # In a production system, stream the file to check size without loading all into memory
    file.file.seek(0, 2)
    file_size = file.file.tell()
    file.file.seek(0)
    if file_size > MAX_FILE_SIZE_BYTES:
        raise HTTPException(status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, f"File size exceeds {MAX_FILE_SIZE_MB}MB limit.")

    document_id = uuid.uuid4()

    # 2. Upload file to S3
    try:
        s3_details = await S3Service.upload_file(
            file=file,
            patient_id=patient_id,
            document_id=str(document_id)
        )
    finally:
        await file.close()

    # 3. Create and save metadata to PostgreSQL
    db_document = ClinicalDocument(
        id=document_id,
        patient_id=patient_id,
        document_name=file.filename,
        document_type=metadata.document_type,
        s3_key=s3_details["s3_key"],
        s3_bucket=s3_details["s3_bucket"],
        file_size_bytes=file_size,
        mime_type=file.content_type,
        status=DocumentStatus.UPLOADED,
        uploaded_by_user_id=current_user.id
    )

    try:
        db.add(db_document)
        await db.commit()
        await db.refresh(db_document)
    except SQLAlchemyError as e:
        # In a real system, you might trigger a cleanup of the orphaned S3 file
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, f"Database error: {e}")

    # 4. Log audit event for PHI creation
    log_audit_event(
        event=AuditEvent.PHI_CREATE,
        user_id=current_user.id,
        client_ip=request.client.host,
        trace_id=trace_id,
        details={
            "entity": "ClinicalDocument",
            "entityId": str(db_document.id),
            "patientId": patient_id,
            "documentName": db_document.document_name,
            "s3_key": db_document.s3_key
        }
    )

    return db_document


# /main.py

from fastapi import FastAPI, Request, status
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import uuid

from core.config import settings
from api.v1.endpoints import clinical_documents
from services.audit_logger import log_audit_event, AuditEvent

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="Backend services for a modern, HIPAA-compliant EMR system.",
    openapi_url=f"{settings.API_V1_STR}/openapi.json",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[str(origin) for origin in settings.BACKEND_CORS_ORIGINS],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    """Attach a unique ID to each request for tracing."""
    request.state.trace_id = str(uuid.uuid4())
    response = await call_next(request)
    response.headers["X-Request-ID"] = request.state.trace_id
    return response


# Exception Handlers
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Custom handler for pydantic validation errors."""
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={"detail": "Validation error", "errors": exc.errors()},
    )


@app.exception_handler(status.HTTP_403_FORBIDDEN)
async def forbidden_exception_handler(request: Request, exc: Exception):
    """Audit log authorization failures."""
    # Attempt to get user ID if token exists, even if invalid, for logging
    auth_header = request.headers.get("Authorization", "Bearer anonymous")
    user_id = auth_header.split(" ")[-1][:10] + "..."  # Log partial token for correlation

    log_audit_event(
        event=AuditEvent.AUTHORIZATION_FAILURE,
        user_id=user_id,
        client_ip=request.client.host,
        trace_id=request.state.trace_id,
        details={
            "reason": "User does not have required role.",
            "required_roles": getattr(exc, 'allowed_roles', 'unknown'),
            "path": request.url.path
        }
    )
    return JSONResponse(
        status_code=status.HTTP_403_FORBIDDEN,
        content={"detail": "The user does not have adequate privileges."},
    )


# API Routers
app.include_router(
    clinical_documents.router,
    prefix=settings.API_V1_STR,
    tags=["Clinical Documents"]
)


@app.get("/", include_in_schema=False)
def read_root():
    return {"status": "ok", "service": settings.APP_NAME, "version": settings.APP_VERSION}


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.ENVIRONMENT == "development",
        log_level="info"
    )

```